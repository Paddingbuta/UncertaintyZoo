"""
topologyuq.py

Uncertainty Method: TopologyUQ (Topological Uncertainty Quantification)

This method quantifies uncertainty based on the topological divergence
of multiple Chain-of-Thought (CoT) reasoning traces generated by a 
generative model, combined with semantic evaluation by a classification model.

Steps:
1. Generate multiple CoT reasoning chains T_i = [s_1, s_2, ..., s_k]
   using a generative model (e.g., ChatGLM) with prompts.

2. Encode each sentence s_j in each chain using a sentence embedding model 
   (e.g., a fine-tuned CodeBERT or Sentence-BERT) to obtain vector trajectories
   V_i = [v_1, v_2, ..., v_k].

3. Perform persistent homology (topological data analysis) on each V_i to
   obtain Persistence Diagrams (PD_i) using tools like gudhi or ripser.

4. Compute pairwise Wasserstein distances W(PD_i, PD_j) between all PDs.

5. Aggregate the distances (e.g., average) as the uncertainty score â€” 
   higher distance indicates more divergent reasoning and higher uncertainty.

Requirements:
- Generative model for CoT reasoning generation (e.g., ChatGLM)
- Embedding model for sentence vectors (e.g., fine-tuned CodeBERT)
- Topological data analysis library (e.g., gudhi, ripser)
- Optimal transport library for Wasserstein distance (e.g., POT)

Example usage:
```python
from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoModel
from uncertainty.methods.topologyuq import TopologyUQ

# Initialize generative model (ChatGLM)
gen_tokenizer = AutoTokenizer.from_pretrained("THUDM/chatglm3-6b", trust_remote_code=True)
gen_model = AutoModelForCausalLM.from_pretrained("THUDM/chatglm3-6b", trust_remote_code=True, device_map="auto")

# Initialize embedding/classification model (CodeBERT fine-tuned)
embed_tokenizer = AutoTokenizer.from_pretrained("./model/codebert-finetuned")
embed_model = AutoModelForSequenceClassification.from_pretrained("./model/codebert-finetuned")

uq = TopologyUQ(
    gen_model=gen_model,
    gen_tokenizer=gen_tokenizer,
    embed_model=embed_model,
    embed_tokenizer=embed_tokenizer,
    device="cuda"
)

code_snippet = "def foo(): ..."

score = uq.quantify(code_snippet)
print("TopologyUQ uncertainty score:", score)


"""

import torch
import numpy as np
from sklearn.metrics import pairwise_distances
import ripser # persistent homology library
from scipy.optimize import linear_sum_assignment

class TopologyUQ:
    def __init__(
        self,
        gen_model,
        gen_tokenizer,
        embed_model,
        embed_tokenizer,
        device="cpu",
        n_samples=10,
        max_steps=10,
        batch_size=2
    ):
        """
        Args:
        gen_model: generative model for CoT reasoning (e.g. ChatGLM)
        gen_tokenizer: tokenizer for generative model
        embed_model: embedding/classification model for sentences (e.g. CodeBERT)
        embed_tokenizer: tokenizer for embedding model
        device: device for inference
        n_samples: number of CoT chains to generate
        max_steps: max reasoning steps per chain
        batch_size: batch size for embedding computation
        """
        self.gen_model = gen_model
        self.gen_tokenizer = gen_tokenizer
        self.embed_model = embed_model
        self.embed_tokenizer = embed_tokenizer
        self.device = device
        self.n_samples = n_samples
        self.max_steps = max_steps
        self.batch_size = batch_size

    def generate_cot_chains(self, code_str):
        """
        Generate multiple CoT reasoning chains from generative model.

        Returns:
            List[List[str]]: List of reasoning chains; each chain is a list of sentences.
        """
        prompt_base = f"Does the following code contain a vulnerability?\n{code_str}\nLet's think step by step:\n"
        cot_chains = []

        self.gen_model.eval()
        for _ in range(self.n_samples):
            inputs = self.gen_tokenizer(prompt_base, return_tensors="pt").to(self.device)
            with torch.no_grad():
                outputs = self.gen_model.generate(
                    **inputs,
                    max_new_tokens=200,
                    do_sample=True,
                    temperature=0.7,
                    num_return_sequences=1,
                    eos_token_id=self.gen_tokenizer.eos_token_id,
                )
            text = self.gen_tokenizer.decode(outputs[0], skip_special_tokens=True)
            # Extract reasoning steps (split by newlines or sentences heuristically)
            # Here, naive splitting by '.', '!', '?' and newline for demo
            import re
            steps = re.split(r'\n|[.!?]', text)
            steps = [s.strip() for s in steps if len(s.strip()) > 5]  # filter very short fragments
            cot_chains.append(steps[:self.max_steps])
        return cot_chains

    def embed_sentences(self, sentences):
        """
        Embed a list of sentences into vectors using embedding model.

        Args:
            sentences (List[str])

        Returns:
            np.ndarray: shape (len(sentences), embedding_dim)
        """
        self.embed_model.eval()
        embeddings = []
        for i in range(0, len(sentences), self.batch_size):
            batch = sentences[i:i+self.batch_size]
            inputs = self.embed_tokenizer(batch, return_tensors="pt", padding=True, truncation=True).to(self.device)
            with torch.no_grad():
                outputs = self.embed_model(**inputs)
                # If model is classification, get last hidden state or pooled output
                # Here we use pooled output if available, else mean of last hidden states
                if hasattr(outputs, "pooler_output") and outputs.pooler_output is not None:
                    emb = outputs.pooler_output.cpu().numpy()
                else:
                    last_hidden = outputs.last_hidden_state  # [bs, seq_len, hidden]
                    emb = last_hidden.mean(dim=1).cpu().numpy()
            embeddings.append(emb)
        return np.vstack(embeddings)

    def persistence_diagram(self, traj):
        """
        Compute persistence diagram from a vector trajectory (time series of embeddings).

        Args:
            traj (np.ndarray): shape (k, d), vector trajectory of reasoning chain steps

        Returns:
            np.ndarray: persistence diagram points [[birth, death], ...]
        """
        # Compute Vietoris-Rips persistence diagram (H0 and H1)
        diagrams = ripser.ripser(traj, maxdim=1)["dgms"]
        # For uncertainty quantification, we can consider H0 + H1 diagrams concatenated
        pd = np.vstack(diagrams) if len(diagrams) > 0 else np.empty((0,2))
        return pd

    def wasserstein_distance(self, pd1, pd2):
        """
        Compute Wasserstein distance between two persistence diagrams.

        Args:
            pd1, pd2: np.ndarray of shape (N1, 2) and (N2, 2)

        Returns:
            float: Wasserstein distance
        """
        # Handle empty diagrams
        if pd1.shape[0] == 0 and pd2.shape[0] == 0:
            return 0.0
        if pd1.shape[0] == 0:
            return np.sum(pd2[:,1] - pd2[:,0])
        if pd2.shape[0] == 0:
            return np.sum(pd1[:,1] - pd1[:,0])

        # Compute pairwise cost matrix between points
        cost_matrix = pairwise_distances(pd1, pd2, metric='euclidean')

        # Solve optimal matching using Hungarian algorithm (linear sum assignment)
        row_ind, col_ind = linear_sum_assignment(cost_matrix)

        dist = cost_matrix[row_ind, col_ind].sum()

        # Add diagonal costs (points matched to diagonal)
        unmatched1 = pd1.shape[0] - len(row_ind)
        unmatched2 = pd2.shape[0] - len(col_ind)
        diag_cost1 = np.sum(pd1[:,1] - pd1[:,0]) if unmatched1 > 0 else 0
        diag_cost2 = np.sum(pd2[:,1] - pd2[:,0]) if unmatched2 > 0 else 0

        return dist + diag_cost1 + diag_cost2

    def quantify(self, code_str):
        """
        Compute the TopologyUQ uncertainty score for a given input.

        Args:
            code_str (str): input source code snippet

        Returns:
            float: aggregated Wasserstein distance (uncertainty score)
        """
        # Step 1: Generate CoT chains
        cot_chains = self.generate_cot_chains(code_str)

        # Step 2: Embed each chain's steps into vectors
        embeddings_chains = []
        for chain in cot_chains:
            if len(chain) == 0:
                # Empty chain, skip
                continue
            emb = self.embed_sentences(chain)  # shape: (steps, emb_dim)
            embeddings_chains.append(emb)

        # Step 3: Compute persistence diagrams for each chain
        pds = [self.persistence_diagram(emb) for emb in embeddings_chains]

        # Step 4: Compute pairwise Wasserstein distances and average
        n = len(pds)
        if n < 2:
            return 0.0  # no uncertainty from single chain

        dists = []
        for i in range(n):
            for j in range(i+1, n):
                d = self.wasserstein_distance(pds[i], pds[j])
                dists.append(d)

        score = np.mean(dists) if dists else 0.0
        return float(score)
